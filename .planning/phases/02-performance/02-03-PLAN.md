---
phase: 02-performance
plan: 03
type: execute
wave: 2
depends_on:
  - 02-01
files_modified:
  - src/transfer/resume.rs
  - src/transfer/compress.rs
  - src/transfer/throttle.rs
  - src/transfer/mod.rs
  - src/transfer/parallel.rs
  - tests/integration_phase2.rs
autonomous: true
requirements:
  - CORE-03
  - CORE-08
  - PERF-03

must_haves:
  truths:
    - "User can resume an interrupted transfer without re-copying completed chunks"
    - "User can enable zstd compression with --compress flag"
    - "User can limit bandwidth with --limit 10MB/s"
    - "Resume manifest (.flux-resume.json) is saved after each chunk and cleaned up on completion"
    - "Bandwidth limit is shared across all parallel chunks (not per-chunk)"
  artifacts:
    - path: "src/transfer/resume.rs"
      provides: "TransferManifest with save/load/cleanup and resume orchestration"
      contains: "pub struct TransferManifest"
    - path: "src/transfer/compress.rs"
      provides: "compress_chunk and decompress_chunk using zstd"
      contains: "pub fn compress_chunk"
    - path: "src/transfer/throttle.rs"
      provides: "ThrottledReader and ThrottledWriter with token-bucket bandwidth limiting"
      contains: "pub struct ThrottledReader"
  key_links:
    - from: "src/transfer/mod.rs"
      to: "src/transfer/resume.rs"
      via: "execute_copy checks for existing manifest on --resume, saves manifest during chunked copy"
      pattern: "TransferManifest"
    - from: "src/transfer/parallel.rs"
      to: "src/transfer/throttle.rs"
      via: "parallel_copy_chunked wraps I/O in throttled reader/writer when bandwidth limit set"
      pattern: "ThrottledReader|throttle"
    - from: "src/transfer/parallel.rs"
      to: "src/transfer/compress.rs"
      via: "Chunks compressed/decompressed when --compress is active"
      pattern: "compress_chunk|decompress_chunk"
---

<objective>
Implement transfer resumability via persistent manifests, zstd compression for text-heavy transfers, and token-bucket bandwidth throttling. These three features complete Phase 2's performance capabilities.

Purpose: Resume prevents wasted work on interrupted large transfers. Compression reduces data volume for compressible content. Throttling prevents saturating shared network links.

Output: Working resume with manifest files, zstd compression wrappers, bandwidth-limited I/O, integrated into the existing parallel copy pipeline.
</objective>

<execution_context>
@C:/Users/trima/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/trima/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-performance/02-RESEARCH.md
@.planning/phases/02-performance/02-01-SUMMARY.md
@src/transfer/chunk.rs
@src/transfer/parallel.rs
@src/transfer/mod.rs
@src/cli/args.rs
@src/error.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Resume manifest and resume orchestration</name>
  <files>
    src/transfer/resume.rs
    src/transfer/mod.rs
    src/transfer/parallel.rs
  </files>
  <action>
1. **src/transfer/resume.rs** -- Create the resume manifest module:

   Define `TransferManifest` struct (derive Debug, Serialize, Deserialize):
   - `version: u32` (set to 1)
   - `source: PathBuf`
   - `dest: PathBuf`
   - `total_size: u64`
   - `chunk_count: usize`
   - `chunks: Vec<ChunkPlan>` (reuse from chunk.rs)
   - `compress: bool`
   - `file_checksum: Option<String>`

   Implement methods on TransferManifest:
   - `manifest_path(dest: &Path) -> PathBuf`: Returns `dest_filename.flux-resume.json` in same directory as dest. Use `dest.with_file_name()` pattern -- get file_name, append ".flux-resume.json" to it, place in dest's parent dir.
   - `save(&self, dest: &Path) -> Result<(), FluxError>`: Serialize to JSON with `serde_json::to_string_pretty`, write to manifest_path. Flush/sync for crash safety.
   - `load(dest: &Path) -> Result<Option<Self>, FluxError>`: If manifest_path exists, read and deserialize. If not, return None. Map serde_json errors to FluxError.
   - `cleanup(dest: &Path) -> Result<(), FluxError>`: Delete manifest file if it exists.
   - `is_compatible(&self, source: &Path, total_size: u64) -> bool`: Check that stored source path matches and total_size matches. If not compatible, resume is invalid.

2. **src/transfer/parallel.rs** -- Extend `parallel_copy_chunked` to support resume:

   Add a new function (or extend the existing one with an optional parameter):
   ```
   pub fn parallel_copy_chunked(
       source: &Path,
       dest: &Path,
       chunks: &mut [ChunkPlan],
       progress: &ProgressBar,
       manifest_dest: Option<&Path>,  // If Some, save manifest after each chunk batch
   ) -> Result<(), FluxError>
   ```

   When `manifest_dest` is Some:
   - After the rayon parallel loop completes (all chunks done), save the manifest. Since rayon processes chunks in parallel, saving per-chunk in the parallel loop would require synchronization. Instead, save the manifest AFTER the par_iter completes. This means if interrupted mid-batch, all chunks in the batch restart, but completed batches are preserved.
   - Alternative simpler approach: Run chunks sequentially in small batches (e.g., batch of N threads), save manifest after each batch. This gives finer-grained resume at the cost of slightly less parallelism. Use this approach -- it's more correct for resume. Specifically: process chunks in groups of `rayon::current_num_threads()`, save manifest after each group.

   For resume: before copying, set progress bar position to sum of completed chunk lengths (so progress reflects already-done work). The `chunks.par_iter_mut().filter(|c| !c.completed)` already skips completed chunks.

3. **src/transfer/mod.rs** -- Integrate resume into execute_copy:

   Add `pub mod resume;` declaration.

   In the single-file copy path of `execute_copy`:
   - If `args.resume`:
     - Call `TransferManifest::load(&final_dest)`
     - If manifest exists and is compatible (same source, same size):
       - Use manifest's chunks (some may be completed)
       - Log "Resuming transfer: {completed}/{total} chunks already done"
       - Set progress bar initial position to sum of completed chunk bytes
     - If manifest exists but NOT compatible:
       - Log warning, delete old manifest, start fresh
     - If no manifest: start fresh, create manifest before starting
   - During chunked copy: pass `Some(&final_dest)` as manifest_dest so manifest is saved
   - After successful completion: call `TransferManifest::cleanup(&final_dest)` to remove the manifest file

   Add unit tests in resume.rs:
   - `save` then `load` roundtrips correctly
   - `manifest_path` produces correct path for various dest paths
   - `cleanup` removes manifest file
   - `is_compatible` returns false when source or size differs
   - `load` returns None when no manifest exists
  </action>
  <verify>
    `cargo test resume` passes all unit tests. `cargo build` succeeds. Manual test: start a copy, Ctrl+C, see .flux-resume.json created, run again with --resume, see it resume.
  </verify>
  <done>
    TransferManifest serializes/deserializes correctly. Manifest is saved during chunked transfer and cleaned up on completion. Resume skips completed chunks and picks up where it left off. Incompatible manifests are detected and discarded.
  </done>
</task>

<task type="auto">
  <name>Task 2: Compression, bandwidth throttling, and integration tests</name>
  <files>
    src/transfer/compress.rs
    src/transfer/throttle.rs
    src/transfer/mod.rs
    src/transfer/parallel.rs
    tests/integration_phase2.rs
  </files>
  <action>
1. **src/transfer/compress.rs** -- Create zstd compression module:

   Implement `compress_chunk(data: &[u8], level: i32) -> Result<Vec<u8>, FluxError>`:
   - Use `zstd::encode_all(std::io::Cursor::new(data), level)` -- wraps data in a Cursor for Read trait
   - Map errors to `FluxError::CompressionError`
   - Default level: 3

   Implement `decompress_chunk(data: &[u8]) -> Result<Vec<u8>, FluxError>`:
   - Use `zstd::decode_all(std::io::Cursor::new(data))`
   - Map errors to `FluxError::CompressionError`

   Add unit tests:
   - Compress then decompress returns original data
   - Compressed text data is smaller than original
   - Empty data roundtrips correctly
   - Random binary data roundtrips (even if not smaller)

2. **src/transfer/throttle.rs** -- Create bandwidth throttling module:

   Implement `parse_bandwidth(s: &str) -> Result<u64, FluxError>`:
   - Strip trailing "/s" or "/S" (case insensitive)
   - Parse remainder with `s.parse::<bytesize::ByteSize>()`
   - Return `.as_u64()` as bytes per second
   - Map parse errors to `FluxError::Config`

   Implement `ThrottledReader<R: Read>`:
   - Fields: `inner: R`, `bytes_per_sec: u64`, `tokens: u64`, `last_refill: Instant`
   - `new(inner: R, bytes_per_sec: u64) -> Self` -- start with 1 second of tokens
   - `refill(&mut self)` -- add tokens based on elapsed time, cap at 2 seconds burst
   - Impl `Read for ThrottledReader<R>`:
     - Call `refill()`
     - If tokens == 0, sleep for `buf.len() / bytes_per_sec` seconds, then refill
     - Limit read size to min(buf.len(), tokens), read from inner, subtract tokens
     - Return bytes read

   Implement `ThrottledWriter<W: Write>`:
   - Same pattern as ThrottledReader but for Write trait
   - `refill`, sleep when no tokens, limit write size to available tokens

   Add unit tests:
   - `parse_bandwidth("10MB/s")` returns 10_000_000 (or 10_485_760 depending on bytesize IEC vs SI -- check bytesize docs and test accordingly)
   - `parse_bandwidth("500KB/s")` returns correct value
   - `parse_bandwidth("invalid")` returns error
   - ThrottledReader limits read speed (test with a fast source, measure elapsed time is >= expected)

3. **src/transfer/mod.rs** -- Add module declarations and integrate:

   Add `pub mod compress;` and `pub mod throttle;` declarations.

   In `execute_copy`, when `args.compress` is set:
   - For now, log a message: "Compression enabled (most effective for network transfers)"
   - For local-to-local: compression adds CPU overhead without reducing I/O. Implement it anyway for correctness, but note it's primarily useful with network backends (Phase 3).
   - When compression is active with chunked copy: read entire chunk -> compress -> write compressed data sequentially (NOT parallel writes, since compressed chunks have variable sizes). Fall back to sequential chunk processing when --compress is active.

   In `execute_copy`, when `args.limit` is set:
   - Parse bandwidth with `parse_bandwidth`
   - Divide limit by chunk_count to get per-thread limit
   - Pass the per-thread limit to `parallel_copy_chunked` which wraps its buffer reads in ThrottledReader (or, simpler: when --limit is set, fall back to sequential chunked copy with a single ThrottledReader wrapping the source reads)
   - Simplest correct approach: when --limit is set, use chunk_count=1 (sequential) with a ThrottledReader wrapping the file read. This avoids the complexity of shared token buckets across threads. Note this in a comment as a Phase 3 optimization opportunity (shared limiter across parallel threads).

4. **tests/integration_phase2.rs** -- Add integration tests (append to existing file from Plan 02):

   - `test_resume_interrupted_transfer`: Create a partial .flux-resume.json manifest with some chunks marked complete, run `flux cp --resume source dest`, verify it doesn't re-copy completed chunks (check that dest is correct)
   - `test_compress_flag`: Run `flux cp --compress source dest`, verify dest matches source content
   - `test_limit_flag`: Run `flux cp --limit 1MB/s source dest` on a small file, verify dest matches source (don't assert timing for CI reliability, just verify correctness)
   - `test_resume_no_manifest`: Run `flux cp --resume source dest` with no manifest, verify it copies normally (no error)
   - `test_all_flags_together`: Run `flux cp --chunks 2 --verify --resume source dest`, verify success
  </action>
  <verify>
    `cargo test` passes ALL tests. `cargo test compress` passes. `cargo test throttle` passes. `cargo test integration_phase2` passes all integration tests including new ones. `cargo build` succeeds with no errors.
  </verify>
  <done>
    User can resume interrupted transfers with --resume (completed chunks skipped, manifest cleaned up on success). User can compress with --compress (zstd roundtrip verified). User can limit bandwidth with --limit (parse and throttle verified). All flags work individually and in combination. Integration tests prove end-to-end correctness. All existing tests pass.
  </done>
</task>

</tasks>

<verification>
1. `cargo test` passes all tests (existing + Plan 02 tests + Plan 03 tests)
2. `flux cp --resume source dest` resumes from manifest when available
3. `flux cp --compress source dest` copies correctly with compression enabled
4. `flux cp --limit 10MB/s source dest` copies with bandwidth limiting
5. `.flux-resume.json` manifest created during transfer, cleaned up on success
6. All flags work in combination: `flux cp --chunks 4 --verify --compress --resume --limit 5MB/s source dest`
7. No regressions in Phase 1 functionality
</verification>

<success_criteria>
- TransferManifest persists chunk state to .flux-resume.json and loads correctly on resume
- Resume skips completed chunks and continues from incomplete ones
- zstd compress/decompress roundtrips produce identical output
- parse_bandwidth correctly handles "10MB/s", "500KB/s", etc.
- ThrottledReader/ThrottledWriter limit throughput via token bucket
- Integration tests prove all features work individually and combined
- All existing tests pass without modification
</success_criteria>

<output>
After completion, create `.planning/phases/02-performance/02-03-SUMMARY.md`
</output>
