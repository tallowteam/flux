---
phase: 02-performance
plan: 02
type: execute
wave: 2
depends_on:
  - 02-01
files_modified:
  - src/transfer/parallel.rs
  - src/transfer/checksum.rs
  - src/transfer/mod.rs
  - src/transfer/copy.rs
  - tests/integration_phase2.rs
autonomous: true
requirements:
  - PERF-01
  - CORE-05

must_haves:
  truths:
    - "User can transfer a large file with parallel chunks using --chunks N"
    - "User can verify transfer integrity with --verify flag (BLAKE3)"
    - "Small files (<10MB) skip chunking and use the existing sequential copy path"
    - "Per-chunk BLAKE3 hashes are computed during transfer"
    - "Post-transfer --verify computes whole-file hash on source and dest and compares them"
  artifacts:
    - path: "src/transfer/parallel.rs"
      provides: "parallel_copy_chunked function using rayon and positional I/O"
      contains: "pub fn parallel_copy_chunked"
    - path: "src/transfer/checksum.rs"
      provides: "hash_file and hash_chunk BLAKE3 functions"
      contains: "pub fn hash_file"
    - path: "src/transfer/mod.rs"
      provides: "Updated execute_copy dispatching to chunked copy when appropriate"
      contains: "parallel_copy_chunked"
  key_links:
    - from: "src/transfer/mod.rs"
      to: "src/transfer/parallel.rs"
      via: "execute_copy calls parallel_copy_chunked for large files or when --chunks specified"
      pattern: "parallel_copy_chunked"
    - from: "src/transfer/parallel.rs"
      to: "src/transfer/checksum.rs"
      via: "Per-chunk hashing during parallel copy"
      pattern: "hash_chunk|blake3"
    - from: "src/transfer/mod.rs"
      to: "src/transfer/checksum.rs"
      via: "Post-transfer --verify whole-file hash comparison"
      pattern: "hash_file.*verify"
---

<objective>
Implement parallel chunked file transfer using rayon and BLAKE3 integrity verification. Files are split into chunks, each copied via positional I/O in parallel threads with per-chunk hashing. The --verify flag adds a post-transfer whole-file hash comparison.

Purpose: Enable users to transfer large files faster through parallel I/O, and verify correctness with cryptographic checksums. This is the core performance feature of Phase 2.

Output: Working parallel chunked copy integrated into the CLI, BLAKE3 checksum module, integration tests proving correctness.
</objective>

<execution_context>
@C:/Users/trima/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/trima/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-performance/02-RESEARCH.md
@.planning/phases/02-performance/02-01-SUMMARY.md
@src/transfer/chunk.rs
@src/transfer/parallel.rs
@src/transfer/mod.rs
@src/transfer/copy.rs
@src/cli/args.rs
@src/error.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: BLAKE3 checksum module and parallel chunked copy</name>
  <files>
    src/transfer/checksum.rs
    src/transfer/parallel.rs
    src/transfer/mod.rs
  </files>
  <action>
1. **src/transfer/checksum.rs** -- Create BLAKE3 checksum module:

   Implement `hash_file(path: &Path) -> Result<String, FluxError>`:
   - Open file, create `blake3::Hasher`, read in 64KB buffer loop, return hex string from `hasher.finalize().to_hex().to_string()`

   Implement `hash_chunk(file: &File, offset: u64, length: u64) -> Result<String, FluxError>`:
   - Use `read_at` from parallel.rs to hash a specific byte range of a file
   - 64KB buffer loop, BLAKE3 Hasher, return hex string

   Add unit tests:
   - `hash_file` returns consistent hash for known content
   - `hash_chunk` of entire file equals `hash_file`
   - `hash_chunk` of different ranges returns different hashes
   - Empty file produces a valid hash

2. **src/transfer/parallel.rs** -- Add `parallel_copy_chunked` function:

   ```
   pub fn parallel_copy_chunked(
       source: &Path,
       dest: &Path,
       chunks: &mut [ChunkPlan],
       progress: &ProgressBar,
   ) -> Result<(), FluxError>
   ```

   Implementation:
   - Open source file as `Arc<File>` (read-only)
   - Create dest file, call `file.set_len(total_size)` to pre-allocate, wrap as `Arc<File>`
   - Use `rayon::prelude::*` with `chunks.par_iter_mut().filter(|c| !c.completed).try_for_each(...)`:
     - For each chunk: allocate 256KB buffer, loop read_at from source -> write_at to dest
     - Compute BLAKE3 hash of each chunk during copy (blake3::Hasher updated with each buffer)
     - Call `progress.inc(n as u64)` after each buffer write
     - After chunk complete: set `chunk.completed = true`, `chunk.checksum = Some(hex_hash)`
   - Important: The destination file must be opened with both read AND write permissions (OpenOptions::new().read(true).write(true).create(true)) since parallel.rs write_at needs write access
   - Return Ok(()) when all chunks complete

   Add unit tests:
   - Copy a known file (e.g., 1MB of pattern data) with 4 chunks, verify dest content matches source byte-for-byte
   - Verify all chunk checksums are populated after copy
   - Copy with 1 chunk works (degenerate case)

3. **src/transfer/mod.rs** -- Add `pub mod checksum;` declaration.

  </action>
  <verify>
    `cargo test checksum` passes. `cargo test parallel` passes (including new parallel_copy_chunked tests). `cargo build` succeeds.
  </verify>
  <done>
    hash_file and hash_chunk produce correct BLAKE3 hashes. parallel_copy_chunked copies files correctly using rayon with per-chunk hashing. Destination file content matches source byte-for-byte after parallel chunked copy. All chunk checksums are populated.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate chunked copy into execute_copy with --verify support</name>
  <files>
    src/transfer/mod.rs
    src/transfer/copy.rs
    tests/integration_phase2.rs
  </files>
  <action>
1. **src/transfer/mod.rs** -- Update `execute_copy` to dispatch to parallel chunked copy:

   After existing validation and before the `if source_meta.is_file()` block, determine the chunk strategy:
   - If `args.chunks > 0`: use that as chunk count (user explicit)
   - Else: call `auto_chunk_count(source_meta.len())` to determine
   - If determined chunk count is 1 AND compression/throttling are not active: use existing `copy_file_with_progress` (no overhead for small files)
   - If chunk count > 1: use `parallel_copy_chunked`

   For single-file path when using chunked copy:
   - Call `chunk_file(size, chunk_count)` to get chunks
   - Create byte-level progress bar (not file-count) using `create_file_progress`
   - Call `parallel_copy_chunked(source, &final_dest, &mut chunks, &progress)`
   - After copy completes, if `args.verify`:
     - Compute `hash_file(source)` and `hash_file(&final_dest)`
     - Compare: if mismatch, return `FluxError::ChecksumMismatch`
     - If match, log "Integrity verified (BLAKE3)" at info level

   For directory copy when using chunked copy:
   - Pass chunk settings through to `copy_directory` (add `chunks: usize` and `verify: bool` parameters)
   - In `copy_directory`, for each file: if file_size triggers chunking, use `parallel_copy_chunked` instead of `copy_file_with_progress`
   - After directory copy, if `--verify` is set, perform per-file hash verification on all copied files (collect mismatches in TransferResult.errors)

   Note: The `quiet` parameter for `execute_copy` should flow through to suppress progress bars and verification messages.

2. **tests/integration_phase2.rs** -- Create integration tests:

   Using `assert_cmd::Command::cargo_bin("flux")` and `tempfile::TempDir`:

   - `test_chunks_flag_parallel_copy`: Create a 1MB file with known pattern, run `flux cp --chunks 4 source dest`, verify dest matches source
   - `test_verify_flag_passes`: Copy a file with `--verify`, verify exit code 0 and stderr contains "verified"
   - `test_auto_chunk_small_file`: Copy a small file (<10MB) without --chunks, verify it works (auto-detects 1 chunk, uses sequential path)
   - `test_chunks_one_sequential`: Copy with `--chunks 1`, verify it works (explicit single chunk)
   - `test_directory_with_chunks`: Create directory with multiple files, `flux cp -r --chunks 2 src/ dest/`, verify all files copied correctly
   - `test_verify_directory`: Copy directory with `--verify`, verify exit code 0

   Pattern for creating test files: write a known byte pattern (e.g., repeating 0..255 to reach desired size) so content is deterministic and verifiable.
  </action>
  <verify>
    `cargo test` passes ALL tests (existing + new). `cargo test integration_phase2` passes all 6 new integration tests. `cargo run -- cp --chunks 4 --verify <source> <dest>` works end-to-end on a real file.
  </verify>
  <done>
    User can run `flux cp --chunks 4 large_file dest` and the file is copied using 4 parallel chunks. User can run `flux cp --verify source dest` and see "Integrity verified (BLAKE3)" on success. Small files automatically skip chunking. Directory copies work with --chunks and --verify. All existing tests continue to pass.
  </done>
</task>

</tasks>

<verification>
1. `cargo test` passes all tests (existing 36 + new unit tests + integration tests)
2. `flux cp --chunks 4 source dest` copies a file using parallel chunks
3. `flux cp --verify source dest` reports integrity verification pass
4. Small files (<10MB) with default --chunks=0 use sequential copy (no overhead)
5. Directory copy with --chunks and --verify works correctly
6. No regressions in existing Phase 1 functionality
</verification>

<success_criteria>
- parallel_copy_chunked correctly copies files using rayon with N parallel threads
- BLAKE3 hash_file and hash_chunk produce correct, consistent hashes
- execute_copy dispatches to chunked copy when chunks > 1
- --verify flag triggers post-transfer whole-file BLAKE3 comparison
- Small files fall back to sequential copy_file_with_progress
- Integration tests prove end-to-end correctness with CLI flags
- All existing tests pass without modification
</success_criteria>

<output>
After completion, create `.planning/phases/02-performance/02-02-SUMMARY.md`
</output>
